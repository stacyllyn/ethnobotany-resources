---
output: github_document
---


```{r, include = FALSE}

packages <- c("rvest", "xml2", "dplyr", "purrr", "stringr",
              "rmarkdown", "knitr", "xml2", "data.table", "httr",
              "leaflet")

install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

invisible(lapply(packages, install_if_missing))
invisible(lapply(packages, library, character.only = TRUE))

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```
	
# Scraping BM-EB

```{r scrape-bmeb}

# --- Install required packages only if missing ---


# --- Define the URL ---
url <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=list&o=1"

# --- Read the HTML content ---
page <- read_html(url)

# --- Extract all <table> elements ---
tables <- page %>% html_nodes("table")

# --- Check how many tables are found ---
length(tables)

# --- Convert the first table to a data frame ---
ethno_table <- tables[[1]] %>% html_table(fill = TRUE)

# --- Preview the data ---
glimpse(ethno_table)

# Optional: View first rows
head(ethno_table)

# --- Create 'data' directory if it doesn't exist ---
if (!dir.exists("data")) {
  dir.create("data")
}

# --- Save the table as CSV in the 'data' directory ---
write.csv(ethno_table, file = "data/ethno_table.csv", row.names = FALSE)


```


## Display BM EB Table

```{r bmeb-table}

bmeb <- read.csv("data/ethno_table.csv")

kable(bmeb, caption = "Ethnobotany plants in Bishop Museum")


```

# Webcrawler

Coded with assistance from ChatGPT. 

Currently should have the following functionality: 

* Scrapes all pages with pagination,
* Pulls all depth-2 details,
* Normalizes them into a single `data.table`,
* Polite throttling included, and
* Can resume if interrupted.

```{r d1crawl, eval = TRUE}

                                        # Base URL (pagination placeholder)
base_url_template <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=list&o=%d"

                                        # Output files for resume
depth1_file <- "data/ethnobotany_depth1_progress.csv"
final_file  <- "data/ethnobotany_full_normalized.csv"

                                        # ---- Step 1: Load existing depth-1 data if resuming ----
if (file.exists(depth1_file)) {
    depth1_dt <- fread(depth1_file)
    message("Resuming with ", nrow(depth1_dt), " depth-1 rows already scraped.")
} else {
    depth1_dt <- data.table()
}

                                        # ---- Step 2: Scrape depth-1 pages ----
scrape_depth1_page <- function(page_number) {
    url <- sprintf(base_url_template, page_number)
    message("Scraping depth-1 page: ", url)

    page <- tryCatch(read_html(url), error = function(e) return(NULL))
    if (is.null(page)) return(NULL)

    tbl_node <- page %>% html_node("table")
    if (is.na(tbl_node) || length(tbl_node) == 0) return(NULL)

    main_table <- tbl_node %>% html_table(fill = TRUE)
    if (nrow(main_table) == 0) return(NULL)

    dt <- as.data.table(main_table)
    setnames(dt, make.names(names(dt), unique = TRUE))

                                        # Hawaiian Name links
    hawaiian_nodes <- page %>%
        html_nodes("table tr td:nth-child(1) a")

    hawaiian_links <- hawaiian_nodes %>%
        html_attr("href") %>%
        url_absolute(url)

    dt[, Detail_URL := hawaiian_links]

    Sys.sleep(runif(1, 1, 3))  # polite random pause
    return(dt)
}

                                        # If no depth-1 data yet, start scraping from page 1
if (nrow(depth1_dt) == 0) {
    all_depth1 <- list()
    page_number <- 1

    repeat {
        dt <- scrape_depth1_page(page_number)
        if (is.null(dt)) break
        all_depth1[[length(all_depth1) + 1]] <- dt
        depth1_dt <- rbindlist(all_depth1, fill = TRUE)
        fwrite(depth1_dt, depth1_file)  # save progress
        page_number <- page_number + 1
    }
}

                                        # ---- Step 3: Scrape depth-2 detail pages ----
scrape_detail_page <- function(url) {
    message("   Depth-2: ", url)

    page <- tryCatch(read_html(url), error = function(e) return(NULL))
    if (is.null(page)) return(list())

    detail_tables <- page %>%
        html_nodes("table") %>%
        html_table(fill = TRUE)

    if (length(detail_tables) == 0) return(list())

    detail_df <- detail_tables[[1]]

    if (ncol(detail_df) >= 2) {
        labels <- str_trim(detail_df[[1]])
        values <- str_trim(detail_df[[2]])
        names(values) <- make.names(labels, unique = TRUE)
        values <- as.list(values)
    } else {
        values <- list()
    }

    Sys.sleep(runif(1, 1, 3))  # polite random pause
    return(values)
}

                                        # ---- Step 4: Resume-aware depth-2 scraping ----
if (file.exists(final_file)) {
    final_dt <- fread(final_file)
    scraped_urls <- unique(final_dt$Detail_URL)
} else {
    final_dt <- NULL
    scraped_urls <- character()
}

                                        # Filter only URLs we haven't scraped yet
pending_urls <- setdiff(depth1_dt$Detail_URL, scraped_urls)
message("Depth-2 pages remaining: ", length(pending_urls))

                                        # Scrape only pending detail pages
detail_list <- map(pending_urls, scrape_detail_page)

                                        # Convert to data.table
details_dt <- rbindlist(detail_list, fill = TRUE)

                                        # Match them back to depth1 rows
new_data <- depth1_dt[Detail_URL %in% pending_urls]
final_batch <- cbind(new_data, details_dt)

                                        # Append to final dataset
if (!is.null(final_dt)) {
    final_dt <- rbindlist(list(final_dt, final_batch), fill = TRUE)
} else {
    final_dt <- final_batch
}


                                        # Save progress
fwrite(final_dt, final_file)

                                        # Prep final_dt for depth 2 crawling
colnames(final_dt)[grepl("URL", colnames(final_dt))] <- "URL"
final_dt[["depth"]] <- rep(2, dim(final_dt)[1])

```


# Depth 2 Crawler

```{r helpers_scrape, message=FALSE, warning=FALSE}

# Text helpers
clean_txt <- function(x) {
  x <- gsub("\u00A0", " ", x)          # NBSP -> space
  x <- gsub("\\s+", " ", x)
  stringr::str_squish(x)
}
esc_re <- function(x) gsub("([][{}()+*^$|?.\\-\\\\])", "\\\\\\1", x)

# Canonical label mapping (extend as needed)
canon_label <- function(label) {
  L <- tolower(clean_txt(gsub("[:]+\\s*$", "", label)))
  patterns <- list(
    "Hawaiian Name(s)" = "^hawaiian\\s+name(s)?$",
    "Scientific Name"  = "^scientific\\s+name$|^species$|^binomial$|^taxon$",
    "Vernacular Name"  = "^vernacular\\s+name(s)?$|^common\\s+name(s)?$|^english\\s+name(s)?$",
    "Family"           = "^family$",
    "Status"           = "^status$|^native\\s*status$|^endemic( status)?$",
    "Part Used"        = "^part(s)?\\s+used$",
    "Uses"             = "^use(s)?$|^traditional\\s+use(s)?$|^uses(/|\\s+)practices$",
    "Medicinal"        = "^medicinal$|^medicine$",
    "Habitat"          = "^habitat$",
    "Distribution"     = "^distribution$|^location(s)?$|^island(s)?$",
    "References"       = "^reference(s)?$|^sources?$",
    "Notes"            = "^note(s)?$|^remarks$"
  )
  for (k in names(patterns)) if (grepl(patterns[[k]], L, perl = TRUE)) return(k)
  # Fallback: keep original label (trimmed)
  sub(":\\s*$", "", clean_txt(label))
}

# Extract label/value pairs from a content node
extract_fields <- function(content_node) {
  fields <- list()
  addf <- function(label, value) {
    lbl <- canon_label(label)
    val <- clean_txt(value)
    if (!nzchar(lbl) || !nzchar(val)) return(invisible())
    if (is.null(fields[[lbl]])) fields[[lbl]] <- val
    else fields[[lbl]] <- paste(fields[[lbl]], val, sep = "\n")
  }

  # 1) Definition lists: <dt>Label</dt><dd>Value</dd>
  for (dl in rvest::html_elements(content_node, "dl")) {
    dts <- rvest::html_elements(dl, "dt"); dds <- rvest::html_elements(dl, "dd")
    n <- min(length(dts), length(dds))
    for (i in seq_len(n)) addf(rvest::html_text2(dts[[i]]), rvest::html_text2(dds[[i]]))
  }

  # 2) Tables: assume first cell is label, rest are value
  for (tbl in rvest::html_elements(content_node, "table")) {
    for (tr in rvest::html_elements(tbl, "tr")) {
      cells <- rvest::html_elements(tr, "th, td")
      if (length(cells) >= 2) {
        label <- rvest::html_text2(cells[[1]])
        vals <- vapply(cells[-1], rvest::html_text2, "", USE.NAMES = FALSE)
        vals <- vals[nzchar(vals)]
        if (length(vals)) addf(label, paste(vals, collapse = " | "))
      }
    }
  }

  # 3) Bold/strong label at start of paragraph/list item
  for (parent in rvest::html_elements(content_node, "p, li")) {
    b <- rvest::html_element(parent, "strong, b")
    if (!inherits(b, "xml_missing") && length(b) > 0) {
      label <- clean_txt(rvest::html_text2(b))
      par_txt <- rvest::html_text2(parent)
      if (nzchar(label) && nzchar(par_txt)) {
        pattern <- paste0("^\\s*", esc_re(label), "\\s*:?\\s*")
        value <- clean_txt(sub(pattern, "", par_txt, perl = TRUE))
        if (nzchar(value)) addf(label, value)
      }
    }
  }

  # 4) Plain "Label: Value" lines (no bold)
  known_left <- c("Hawaiian Name", "Hawaiian Name(s)", "Scientific Name", "Vernacular Name",
                  "Common Name", "English Name", "Family", "Status", "Part Used", "Uses",
                  "Medicinal", "Habitat", "Distribution", "References", "Notes")
  left_re <- paste0("^\\s*(", paste(unique(known_left), collapse="|"), ")\\s*:\\s*(.+)$")
  for (node in rvest::html_elements(content_node, "p, li, div")) {
    txt <- rvest::html_text2(node)
    if (!nzchar(txt)) next
    m <- regexec(left_re, txt, ignore.case = TRUE)
    rr <- regmatches(txt, m)[[1]]
    if (length(rr) >= 3) addf(rr[2], rr[3])
  }

  fields
}

# Choose content container
choose_content_node <- function(doc, selectors = c("#content","div#content","#main","main","article",".content","#mw-content-text","body")) {
  for (sel in selectors) {
    node <- rvest::html_element(doc, sel)
    if (!inherits(node, "xml_missing") && length(node) > 0) return(node)
  }
  rvest::html_element(doc, "body")
}

# Scrape one URL -> one-row data.table (wide)
scrape_one <- function(url,
                       ua = "ethnobotany-resources/0.1 (+https://github.com/stacyllyn/ethnobotany-resources)",
                       timeout_sec = 30) {
  safe_row <- function(error = NA_character_, status = NA_integer_, title = NA_character_) {
    data.table(url = url, page_title = title, status = status,
               fetched_at = Sys.time(), error = error)
  }

  resp <- tryCatch(httr::GET(url, httr::user_agent(ua), httr::timeout(timeout_sec)), error = identity)
  if (inherits(resp, "error")) return(safe_row(paste("request_error:", resp$message)))
  status <- httr::status_code(resp)
  if (status >= 400) return(safe_row(paste("http_status", status), status = status))

  html_txt <- tryCatch(httr::content(resp, as = "text", encoding = "UTF-8"), error = identity)
  if (inherits(html_txt, "error") || !nzchar(html_txt)) return(safe_row("empty_response", status = status))

  doc <- tryCatch(xml2::read_html(html_txt), error = identity)
  if (inherits(doc, "error")) return(safe_row("parse_error", status = status))

  title_node <- rvest::html_element(doc, "title")
  page_title <- if (inherits(title_node, "xml_missing")) NA_character_ else clean_txt(rvest::html_text2(title_node))

  content_node <- choose_content_node(doc)
  if (inherits(content_node, "xml_missing") || length(content_node) == 0) {
    return(safe_row("content_not_found", status = status, title = page_title))
  }

  fields <- extract_fields(content_node)

  cbind(
    data.table(url = url, page_title = page_title, status = status,
               fetched_at = Sys.time(), error = NA_character_),
    as.data.table(as.list(fields), keep.rownames = FALSE)
  )
}

# Scrape many URLs
scrape_many <- function(urls, sleep_bounds = c(0.2, 0.6)) {
  if (!length(urls)) return(data.table())
  pb <- txtProgressBar(min = 0, max = length(urls), style = 3)
  res <- vector("list", length(urls))
  for (i in seq_along(urls)) {
    Sys.sleep(runif(1, sleep_bounds[1], sleep_bounds[2]))  # polite delay
    res[[i]] <- tryCatch(scrape_one(urls[[i]]), error = function(e) {
      data.table(url = urls[[i]], page_title = NA_character_, status = NA_integer_,
                 fetched_at = Sys.time(), error = paste("unexpected_error:", e$message))
    })
    setTxtProgressBar(pb, i)
  }
  close(pb)
  data.table::rbindlist(res, fill = TRUE, use.names = TRUE)
}

```


```{r diagnose_labels_optional, eval=FALSE}

# Optional diagnostics: peek at first-column labels inside common containers for one URL
u <- "https://data.bishopmuseum.org/ethnobotanydb/ethnobotany.php?b=d&ID=aalii"
doc <- xml2::read_html(u)
sels <- c("#content","div#content","#main","main","article",".content","#mw-content-text","body")
diag <- lapply(sels, function(s) {
  node <- rvest::html_element(doc, s)
  if (inherits(node, "xml_missing") || length(node) == 0) return(NULL)
  rows <- rvest::html_elements(node, "tr")
  if (!length(rows)) return(NULL)
  labs <- unlist(lapply(rows, function(tr) {
    cells <- rvest::html_elements(tr, "th, td")
    if (length(cells) >= 1) rvest::html_text2(cells[[1]]) else NA_character_
  }))
  data.table(selector = s, first_col_labels = unique(na.omit(stringr::str_squish(labs))))
})
Filter(Negate(is.null), diag)

```

```{r run_scrape_depth2, message=FALSE, warning=FALSE}

# Assumes you've already run:
# - setup_deps (packages)
# - functions chunk (defines clean_txt, canon_label, extract_fields, scrape_one, scrape_many)

stopifnot(exists("final_dt"))
url_col   <- intersect(names(final_dt), c("url","URL","link","href"))[1]
depth_col <- intersect(names(final_dt), c("depth","Depth","level","crawl_depth"))[1]
if (is.na(url_col))   stop("final_dt must have a URL column.")
if (is.na(depth_col)) stop("final_dt must have a crawl depth column.")

depth2_urls <- unique(na.omit(final_dt[get(depth_col) == 2][[url_col]]))
message(sprintf("Scraping %d depth-2 URLs â€¦", length(depth2_urls)))

# Execute scraping (polite delays handled in scrape_many/scrape_one)
depth2_wide <- scrape_many(depth2_urls)

# Optional: write results
# data.table::fwrite(depth2_wide, "data/depth2_wide.csv")

# Quick summary of extracted fields
found_cols <- setdiff(names(depth2_wide), c("url","page_title","status","fetched_at","error"))
message(sprintf("Extracted %d distinct field labels.", length(found_cols)))
print(sort(found_cols))

depth2_wide

```


# Leaflet map

```{r leaf-map}

# Create a leaflet map widget
leaflet() %>%
  # Add default OpenStreetMap tiles to the map
  addTiles() %>%
  # Set the initial view of the map
  setView(lng = -157.858333, lat = 21.306944, zoom = 10) %>%
  # Add a marker for a specific location (Honolulu)
  addMarkers(lng = -157.858333, lat = 21.306944, popup = "Aloha from Honolulu! ðŸŒº")

```
